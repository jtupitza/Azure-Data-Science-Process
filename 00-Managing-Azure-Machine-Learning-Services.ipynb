{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing Azure Machine Learning Services\n",
    "Using Azure Machine Learning Services as a platform for developing and operationalizing machine learning services is dependent upon configuring a number of assets that work together to form that platform. Completing this process involves:\n",
    "* Configuring an Azure Machine Learning **Workspace**\n",
    "* Configuring one or more Azure Machine Learning **Dataspaces**, and registering them to the AML Workspace\n",
    "* Configuring one or more Azure Machine Learning **Compute Targets**, and registering them to the AML Workspace\n",
    "* Configuring the execution **Environment** that runs on the Compute targets, and that facilitates the execution of Experiments (runs)\n",
    "\n",
    "The following sections demonstrate executing these steps to enable the **Team Data Science Process** using Azure Machine Learning services. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing Azure Machine Learning Workspaces\n",
    "Creating Azure Machine Learning Services workspaces is a very straight-forward operation having only four (4) parameters:\n",
    "1. An Azure **Subscription ID** (GUID)\n",
    "2. A **Workspace Name** for the new workspace being created\n",
    "3. The name of a new or existing Azure **Resource Group**\n",
    "4. An Azure Datacenter **Location**; e.g., US East, US West 2, North Europe, West Europe\n",
    "\n",
    "What's more, Azure Machine Learning Service workspaces can be managed using a variety of tools and approaches:\n",
    "* Using the Azure Portal interface\n",
    "* Using an Azure Resource Manager (ARM) template\n",
    "* Using the **Azure Machine Learning SDK for Python** from a variety of IDE's:\n",
    "  * Azure Jupyter Notebooks\n",
    "  * Locally installed Jupyter Notebooks\n",
    "  * Visual Studio Code\n",
    "  * Azure Data Science Virtual Machines\n",
    "  * Azure Databricks\n",
    "* Using the **CLI Extension for Azure Machine Learning** from one of the following Command-Line Interfaces:\n",
    "  * Azure CLI\n",
    "  * Azure PowerShell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a New Workspace using the Azure Machine Learning SDK for Python\n",
    "The following steps demonstrate how easily Azure ML workspaces can be managed using Python\n",
    "\n",
    "#### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azureml.core import Workspace\n",
    "from azureml.core import Datastore\n",
    "#from azureml.core.model import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define and Initialize Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const_subscription_id = os.getenv(\"SUBSCRIPTION_ID\", default=\"<your subscription id here>\")\n",
    "\n",
    "workspace_name = os.getenv(\"WORKSPACE_NAME\", default=\"customer-churn\")\n",
    "resource_group_name = os.getenv(\"RESOURCE_GROUP_NAME\", default=\"amls-rg\")\n",
    "create_new_rg = os.getenv(\"CREATE_NEW_RG\", default=True)\n",
    "location_name = os.getenv(\"LOCATION_NAME\", default=\"eastus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the New Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, try to locate an existing Workspace that matches your specifications\n",
    "try:\n",
    "    ws = Workspace(subscription_id = const_subscription_id,\n",
    "                   resource_group = resource_group_name,\n",
    "                   workspace_name = workspace_name)\n",
    "    print('An existing Workspace matching your specification was referenced.')\n",
    "    \n",
    "except:    \n",
    "    ws = Workspace.create(name=workspace_name,\n",
    "                          subscription_id = const_subscription_id,\n",
    "                          resource_group = resource_group_name,\n",
    "                          create_resource_group = create_new_rg,\n",
    "                          location = location_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View Workspace Configuration Details "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws.get_details()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Workspace Configuration Details\n",
    "Saving the new workspace's configuration details to a local file enables loading the same workspace from other Jupyter Notebooks or Python scripts. Since by default the configuration file **.azureml\\config.json** is created in the directory containing the Jupyter Notebook file used to create the workspace, it can only be loaded by Jupyter Notebooks and Python scripts that reside in that directory or its sub-directories. However, by simply copying the **.azureml\\config.json** file to a different directory it can then be used to load this workspace from Jupyter Notebooks and Python scripts residing in that directory or its sub-directories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws.write_config()\n",
    "\n",
    "# Subsequently, the following code is used to load\n",
    "# the workspace from scripts and Jupyter Notebooks\n",
    "\n",
    "# ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing Azure Machine Learning Datastores\n",
    "In Azure Machine Learning services, datastores are managed independently from compute resources to provide a layer of abstraction between those resources. This configuration allows for architectural flexibility by enabling storage resources to be added or removed without requiring any coding changes.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get a Reference to the Workspace's Default Datastore\n",
    "A Datastore is automatically created as part of the Workspace provisioning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ws.get_default_datastore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining a Different Default Datastore for the Current Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws.set_default_datastore('datastore name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enumerate All Datastores Currently Registered in the Current Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastores = ws.datastores\n",
    "for name, ds in datastores.items():\n",
    "    print(name, ds.datastore_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get a Reference to a Specific Datastore Currently Registered in the Current Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Datastore.get(ws, datastore_name='workspaceblobstore')  # datastore name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering New Datastores with the Workspace\n",
    "* Use the **register_azure_blob_container()** method to register an Azure Blob Container with a Workspace\n",
    "* Use the **register_azure_file_share()** method to register an Azure File Share with a Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Datastore.register_azure_blob_container(workspace=ws, datastore_name='datastore name', \n",
    "                                             container_name='azure blob container name',\n",
    "                                             account_name='storage account name', account_key='storage account key',\n",
    "                                             create_if_not_exists=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Datastore.register_azure_file_share(workspace=ws, datastore_name='datastore name', \n",
    "                                         file_share_name='file share name',\n",
    "                                         account_name='storage account name', account_key='storage account key',\n",
    "                                         create_if_not_exists=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Data in Azure Machine Learning Datastores\n",
    "* Use the **upload()** method of the Datastore object to load files from a local source directory to the Datastore\n",
    "* Use the **download()** method of the Datastore object to copy files from the Datastore to a local target directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.data\n",
    "from azureml.data.azure_storage_datastore import AzureFileDatastore, AzureBlobDatastore\n",
    "\n",
    "ds.upload(src_dir='your source directory',\n",
    "          target_path='your remote target path',\n",
    "          overwrite=True, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.download(target_path='your local target path',\n",
    "            prefix='your prefix', show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing Datastores for Training and Evaluating Machine Learning Models\n",
    "* Use the **as_mount()** method to mount a Datastore on a compute target\n",
    "* Use the **as_download()** method to download Datastore contents to the location specified by the **path_on_compute** parameter\n",
    "* Use the **as_upload()** method to upload a file to the Datastore from the location specified by the **path_on_compute** parameter\n",
    "* Use the **path()** method to reference a specific folder or file in the Datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.data.data_reference import DataReference\n",
    "\n",
    "ds.as_mount()\n",
    "ds.as_download(path_on_compute='your path on compute')\n",
    "ds.as_upload(path_on_compute='yourfilename')\n",
    "\n",
    "# Download the contents of the `./Data` directory in ds to the compute target\n",
    "ds.path('./Data').as_download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing Azure Machine Learning Compute Targets\n",
    "Implementing the **Team Data Science Process** customarily involves performing activities like exploratory data analysis (EDA), feature selection, dimensionality reduction, and the training and testing of machine learning models. These tasks are often initially undertaken using small samples extracted from data sources, and as such are easily achieved using local resources such as a laptop computer or desktop workstation. As the development cycle progresses it often becomes necessary to perform more computationally expensive operations such as hyper-parameter tuning, training models with larger amounts of data, and cross-validation. Ultimately the production solution must then be deployed to an operational platform capable of scaling to accommodate potentially massive data volumes. To that end, Azure Machine Learning services is capable of using a spectrum of **compute targets** in order to accommodate the development lifecycle. AML **Compute targets** are managed compute infrastructure that enable the rapid provisioning of single to multi-node compute resources. They are created within **Workspace** regions and are available to all authenticated **Workspace** users. They autoscale by default when a job is submitted, and they execute in containerized **Environments** that package all user-specified dependencies. These compute targets include:  \n",
    "* Local Computer\n",
    "* Azure Machine Learning Compute\n",
    "* Remote Virtual Machine\n",
    "* Azure Databricks\n",
    "* Azure Data Lake Analytics\n",
    "* Azure HDInsight\n",
    "* Azure Batch\n",
    "\n",
    "When an Azure Machine Learning services Workspace is created using Python, the local computer is automatically attached as the default compute target. This is true regardless of whether that target is a laptop, a desktop, or a virtual machine running in the cloud; e.g., Azure Notebooks, Data Science Virtual Machine (DSVM). When the time comes to execute Python machine learning experiments at scale, an appropriate compute target can be created, attached and configured so that it contains the Python environment needed to execute those scripts; including all of the dependencies referenced in those scripts.\n",
    "\n",
    "#### Managing Environments and their Dependencies using Run Configurations\n",
    "One primary benefit of Azure Machine Learning services is that code developed on a development computer can then be promoted to a highly scalable compute target, such as an Azure Machine Learning Services cluster, without making any changes to the code. This greatly simplifies the deployment process and promotes adopting a DevOps approach to delivery machine learning and artificial intelligence solutions. \n",
    "\n",
    "* **User-Managed Environments:** When you are using your local development computer as the compute target no further action is needed since you will have already configured it with all the resources (dependecies) required to conduct the experiments; e.g., Scikit-Learn, CNTK, Keras.\n",
    "\n",
    "* **System-Managed Environment:** When your are ready to promote your experiments to a more scalable compute target you must ensure all dependecies are satisfied.  This is most easily and most commonly accomplished using Conda to manage the Python environment, and is assumed by default.  You acccomplish this by using the **CondaDependency** class to add a **conda_dependencies.yml** file to your Workspace.  When you provision a new compute target, the YML file defines the configurations for a new Docker containerized environment that contains all the packages and model dependencies required to execute your Python scripts.\n",
    "\n",
    "What's more, Azure Machine Learning services compute targets can either be created on-demand when a run is scheduled, or they can be created as a persistance resource.\n",
    "\n",
    "* **Run-based Compute Creation:** As the naming implies, run-based (on-demand) compute targets are created as part of the schedule run (execution) and subsequently deleted automatically when the script execution completes. As of the time of this writing, the  run-based creation feature is still in Preview, and doesn't yet support automated hyper-parameter tuning or automated machine learning.  For these reasons I currently recommend creating Persistent compute targets whenever possible.  \n",
    "\n",
    "* **Persistent Compute Creation:** As the naming implies, persistent compute targets are persistently registered with the Azure Machine Learning services Workspace, and are therefore available for use across jobs.  What's more, they can be shared with other users in the Workspace; a capability that helps teams to conserve resources while ensuring consistency on team projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Provision a Persistant Azure Machine Learning Compute Target\n",
    "The following code demonstrates provisioning an Azure Machine Learning cluster as a compute target that will be available to all users of the Workspace for multiple job executions (runs). Setting the **gpu_enabled** parameter to **True** creates a GPU enabled cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "gpu_enabled = False\n",
    "\n",
    "# Choose a name and size for your CPU cluster\n",
    "if gpu_enabled:\n",
    "    cluster_name = \"gpucluster\"\n",
    "    vm_spec = \"STANDARD_NC6\"\n",
    "    \n",
    "else:\n",
    "    cluster_name = \"cpucluster\"\n",
    "    vm_spec = 'STANDARD_D2_V2'\n",
    "    \n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    aml_cluster = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "    \n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size=vm_spec, min_nodes=0, max_nodes=4)\n",
    "    aml_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "aml_cluster.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Create a Run Configuration for the Persistant Compute Target\n",
    "The following code demonstrates how to specify a **system-managed environment** for the persistent Azure Machine Learning **compute target** created in the cell above.  This operation will add all the required packages and other model dependencies to the Docker container in which the AML cluster executes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "\n",
    "# Create a new runconfig object\n",
    "run_amlcompute = RunConfiguration()\n",
    "\n",
    "# Use the cpu_cluster you created above. \n",
    "run_amlcompute.target = cpu_cluster\n",
    "\n",
    "# Enable Docker\n",
    "run_amlcompute.environment.docker.enabled = True\n",
    "\n",
    "# Set Docker base image to the default CPU-based image\n",
    "run_amlcompute.environment.docker.base_image = DEFAULT_CPU_IMAGE\n",
    "\n",
    "# Use conda_dependencies.yml to create a system-managed conda environment in the Docker image\n",
    "run_amlcompute.environment.python.user_managed_dependencies = False\n",
    "\n",
    "# Auto-prepare the Docker image when used for execution (if it is not already prepared)\n",
    "run_amlcompute.auto_prepare_environment = True\n",
    "\n",
    "# Specify CondaDependencies obj, add necessary packages\n",
    "run_amlcompute.environment.python.conda_dependencies = CondaDependencies.create(conda_packages=['scikit-learn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing Azure Machine Learning Runs\n",
    "The preceding sections have described how to provision various resources required to establish an Azure Machine Learning services operating environment; e.g., a Workspace, storage resources, compute resources, and run configurations. Having completed these tasks machine learning experiments can finally be executed; these are referred to as **runs**.\n",
    "\n",
    "#### Create an Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "experiment_name = 'my_experiment'\n",
    "\n",
    "exp = Experiment(workspace=ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submit an Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import ScriptRunConfig\n",
    "import os \n",
    "\n",
    "script_folder = os.getcwd()\n",
    "src = ScriptRunConfig(source_directory = script_folder, script = 'train.py', run_config = run_local)\n",
    "run = exp.submit(src)\n",
    "run.wait_for_completion(show_output = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
